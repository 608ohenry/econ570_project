{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "\n",
    "https://www.cbp.gov/newsroom/stats\n",
    "\n",
    "https://www.ice.gov/detain/detention-management\n",
    "\n",
    "https://www.uscis.gov/tools/reports-and-studies/immigration-and-citizenship-data\n",
    "\n",
    "https://ohss.dhs.gov/topics/immigration#other-resources \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideas\n",
    "- Write about H1B, H2A, H2B, or other visas\n",
    "- Write about detained individual counts at the border\n",
    "- Write about detained individuals within the US\n",
    "- Forecast immigration data\n",
    "- Forecast impacts of policy changes\n",
    "\n",
    "- Combine company visa information with their stock information\n",
    "- Investigate if there are any correlations between stock information and visa information\n",
    "- Can we make price movement predictions with visa information of publicly listed companies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from yfinance import ticker\n",
    "import json\n",
    "from io import StringIO\n",
    "import re\n",
    "from rapidfuzz import process, fuzz\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets a list of all listed companies from SEC's EDGAR database\n",
    "#\"https://www.sec.gov/files/company_tickers.json\"\n",
    "\n",
    "with open('data/company_tickers.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "# companies = [entry[\"ticker\"] for entry in response.values()]\n",
    "# print(companies[:10])  # Print first 10 tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORK IN PROGRESS ###\n",
    "\n",
    "# def read_tsv(file_path):\n",
    "#     \"\"\"Reads a TSV file with UTF-8 encoding.\"\"\"\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         return pd.read_csv(StringIO(f.read()), delimiter=\"\\t\", low_memory=False)\n",
    "    \n",
    "# remove_terms = [\n",
    "#     \"INCORPORATED\", \"INC\", \"CORPORATION\", \"CORP\", \"NA\", \"N A\", \"ENTERTAINMENT\",\n",
    "#     \"CONSULTING\", \"FINANCIAL\", \"FINANCIALS\", \"LOGISTICS\", \"INDUSTRIES\", \"MANAGEMENT\",\n",
    "#     \"LTD\", \"LIMITED\", \"LLC\", \"LLP\", \"PLC\", \"GROUP\", \"GRP\", \"GR\", \"HOLDINGS\", \"COMPANY\",\n",
    "#     \"CO\", \"LP\", \"LL\", \"PARTNERSHIP\", \"TECHNOLOGIES\", \"SYSTEMS\", \"GLOBAL\", \"ENTERPRISES\",\n",
    "#     \"SERVICES\", \"COM\", \"COMMUNICATIONS\", \"SOLUTIONS\", \"INVESTMENT\", \"PHARMACEUTICALS\"\n",
    "# ]\n",
    "# remove_terms_pattern = re.compile(r'\\b(' + '|'.join(remove_terms) + r')\\b')\n",
    "    \n",
    "# def remove_meta_characters(input_string):\n",
    "#     input_string = str(input_string)\n",
    "#     input_string = re.sub(r'\\s+', ' ', input_string).strip() \n",
    "#     input_string = remove_terms_pattern.sub('', input_string)\n",
    "#     input_string = re.sub(r'/[^/]+/', ' ', input_string)\n",
    "#     input_string = input_string.replace('.', '').replace(',', '').replace('&', 'AND')\n",
    "#     input_string = re.sub(r'\\s+', ' ', input_string).strip() \n",
    "#     return input_string\n",
    "\n",
    "# def find_best_match(employer, standard_titles, company_mapping, score_cutoff=75):\n",
    "#     matches = process.extract(employer, standard_titles, scorer=fuzz.token_set_ratio)\n",
    "#     valid_matches = [match for match in matches if match[1] >= score_cutoff]\n",
    "#     if valid_matches:\n",
    "#         best_match_name = max(valid_matches, key=lambda x: x[1])[0]\n",
    "#         return best_match_name, company_mapping.get(best_match_name)\n",
    "#     return None, None\n",
    "\n",
    "# def parallel_find_best_match(employers, standard_titles, company_mapping, n_jobs=-1):\n",
    "#     return Parallel(n_jobs=n_jobs, backend=\"threading\")(\n",
    "#         delayed(find_best_match)(emp, standard_titles, company_mapping) for emp in employers)\n",
    "\n",
    "# titles = [company['title'].upper() for company in data.values()]\n",
    "# standard_titles = [remove_meta_characters(title) for title in titles]\n",
    "# standard_titles = list(set(standard_titles))\n",
    "\n",
    "# company_mapping = {remove_meta_characters(company['title']): company['ticker'] for company in data.values()}\n",
    "\n",
    "# visa_files = {\"h1b\": ['data/visas/h1b_2016_2009.csv', 'data/visas/h1b_2024_2017.csv']}\n",
    "\n",
    "# df_visas = []\n",
    "# for visa_type, files in visa_files.items():\n",
    "#     df_list = [read_tsv(file) for file in files]\n",
    "#     df = pd.concat(df_list, axis=0)\n",
    "#     df[\"employer\"] = df[\"Employer (Petitioner) Name\"].str.upper().apply(remove_meta_characters)\n",
    "#     df.loc[:, \"type\"] = visa_type\n",
    "#     df_visas.append(df)\n",
    " \n",
    "# df_visas_combined = pd.concat(df_visas)\n",
    "\n",
    "# drop_cols = ['Line by line', 'Employer (Petitioner) Name', 'Initial Denial', 'Continuing Denial']\n",
    "# df_visas_clean = df_visas_combined.drop(drop_cols, axis = 1)\n",
    "# df_visas_clean.columns = df_visas_clean.columns.str.strip()\n",
    "\n",
    "# int_columns = [\"Fiscal Year\", \"Tax ID\", \"Petitioner Zip Code\", \"Initial Approval\", \"Continuing Approval\"]\n",
    "\n",
    "# df_visas_clean[int_columns] = df_visas_clean[int_columns].apply(pd.to_numeric, errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "# sum_data = df_visas_clean.groupby([\"Fiscal Year\", \"employer\"], as_index=False).agg({\n",
    "#     \"Tax ID\": \"first\",  \n",
    "#     \"Industry (NAICS) Code\": \"first\",  \n",
    "#     \"Petitioner City\": \"first\",  \n",
    "#     \"Petitioner State\": \"first\",  \n",
    "#     \"Petitioner Zip Code\": \"first\",  \n",
    "#     \"Initial Approval\": \"sum\",  \n",
    "#     \"Continuing Approval\": \"sum\",  \n",
    "#     \"type\": \"first\"  \n",
    "# })\n",
    "\n",
    "# matches = parallel_find_best_match(sum_data[\"employer\"], standard_titles, company_mapping, n_jobs=10)\n",
    "\n",
    "# matches_df = pd.DataFrame(matches, columns=[\"matched_company\", \"ticker\"])\n",
    "# sum_data_filter = pd.concat([sum_data, matches_df], axis=1)\n",
    "\n",
    "# sum_data_filter_narm = sum_data_filter[sum_data_filter[\"matched_company\"].notna()]\n",
    "\n",
    "# final_sum = sum_data_filter_narm.groupby([\"Fiscal Year\", \"ticker\"], as_index=False).agg({\n",
    "#     \"matched_company\": \"first\",\n",
    "#     \"Tax ID\": \"first\",  \n",
    "#     \"Industry (NAICS) Code\": \"first\",  \n",
    "#     \"Petitioner City\": \"first\",  \n",
    "#     \"Petitioner State\": \"first\",  \n",
    "#     \"Petitioner Zip Code\": \"first\",  \n",
    "#     \"Initial Approval\": \"sum\",  \n",
    "#     \"Continuing Approval\": \"sum\",  \n",
    "#     \"type\": \"first\"  \n",
    "# })\n",
    "\n",
    "# final_sum.to_csv(\"data/final_sum.csv\", index=False)\n",
    "\n",
    "# # This code chunk took about 75 minutes to run and complete. My laptop and I are not on speaking terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1b_df = pd.read_csv(\"data/final_sum.csv\")\n",
    "# Note: some large companies are omitted (AAPL, MRST) while others are most likely overestimated due to false positives (AAME, AABQ)\n",
    "# I'll get around to fixing these issues after the midterm, though it should be sufficient for the proposal - Matt\n",
    "\n",
    "# I no longer need the below chunks for debugging. Feel free to remove them if you feel the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_10120\\3969071596.py:13: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(StringIO(f.read()), delimiter=\"\\t\")\n",
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_10120\\3969071596.py:13: DtypeWarning: Columns (9,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(StringIO(f.read()), delimiter=\"\\t\")\n",
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_10120\\3969071596.py:13: DtypeWarning: Columns (4,16,18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(StringIO(f.read()), delimiter=\"\\t\")\n",
      "C:\\Users\\peter\\AppData\\Local\\Temp\\ipykernel_10120\\3969071596.py:13: DtypeWarning: Columns (16,20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  return pd.read_csv(StringIO(f.read()), delimiter=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "h1b_2009_path = 'data/visas/h1b_2016_2009.csv'\n",
    "h1b_2017_path = 'data/visas/h1b_2024_2017.csv'\n",
    "\n",
    "h2a_2015_path = 'data/visas/h2a_2019_2015.csv'\n",
    "h2a_2020_path = 'data/visas/h2a_2024_2020.csv'\n",
    "\n",
    "h2b_2015_path = 'data/visas/h2b_2019_2015.csv'\n",
    "h2b_2020_path = 'data/visas/h2b_2025_2020.csv'\n",
    "\n",
    "#Had to convert all encoding to UTF8 via notepad++\n",
    "def read_tsv(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return pd.read_csv(StringIO(f.read()), delimiter=\"\\t\", low_memory=False)\n",
    "\n",
    "# Read all files using the function\n",
    "df_h1b_a = read_tsv(h1b_2009_path)\n",
    "df_h1b_b = read_tsv(h1b_2017_path)\n",
    "\n",
    "df_h2a_a = read_tsv(h2a_2015_path)\n",
    "df_h2a_b = read_tsv(h2a_2020_path)\n",
    "\n",
    "df_h2b_a = read_tsv(h2b_2015_path)\n",
    "df_h2b_b = read_tsv(h2b_2020_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h1b = pd.concat([df_h1b_a, df_h1b_b], axis=0)\n",
    "\n",
    "df_h2a = pd.concat([df_h2a_a, df_h2a_b], axis=0)\n",
    "\n",
    "df_h2b = pd.concat([df_h2b_a, df_h2b_b], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_h1b.columns)\n",
    "print(df_h2a.columns)\n",
    "print(df_h2b.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_meta_characters(input_string):\n",
    "    input_string = str(input_string)\n",
    "    input_string = re.sub(r'\\s+', ' ', input_string).strip() \n",
    "    input_string = re.sub(r'\\b(INCORPORATED|INC|CORPORATION|CORP|NA|N A|LTD|LIMITED|LLC|LLP|PLC|GROUP|GRP|GR|HOLDINGS|COMPANY|CO|LP|PARTNERSHIP)\\b',\n",
    "                          '', str(input_string).upper())\n",
    "    input_string = re.sub(r'/[^/]+/', ' ', input_string)\n",
    "    input_string = input_string.replace('.', '').replace(',', '').replace('&', 'AND')\n",
    "    input_string = re.sub(r'\\s+', ' ', input_string).strip() \n",
    "    return input_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = [company['title'].upper() for company in data.values()]\n",
    "\n",
    "standard_titles = [remove_meta_characters(title) for title in titles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h1b['employer'] = df_h1b['Employer (Petitioner) Name'].str.upper().apply(remove_meta_characters)\n",
    "df_h2a['employer'] = df_h2a['Employer (Petitioner) Name'].str.upper().apply(remove_meta_characters)\n",
    "df_h2b['employer'] = df_h2b['Employer (Petitioner) Name'].str.upper().apply(remove_meta_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('H1B Crossover Count: ' + str(sum(df_h1b[\"employer\"].isin(standard_titles))))\n",
    "print('H2A Crossover Count: ' + str(sum(df_h2a[\"employer\"].isin(standard_titles))))\n",
    "print('H2B Crossover Count: ' + str(sum(df_h2b[\"employer\"].isin(standard_titles))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_h1b_listed = df_h1b[ df_h1b[\"employer\"].isin(standard_titles) ].copy()\n",
    "df_h1b_listed.loc[:, \"type\"] = 'h1b'\n",
    "\n",
    "df_h2a_listed = df_h2a[ df_h2a[\"employer\"].isin(standard_titles) ].copy()\n",
    "df_h2a_listed.loc[:, \"type\"] = 'h2a'\n",
    "\n",
    "df_h2b_listed = df_h2b[ df_h2b[\"employer\"].isin(standard_titles) ].copy()\n",
    "df_h2b_listed.loc[:, \"type\"] = 'h2b'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_h1b.columns)\n",
    "print(df_h2a.columns)\n",
    "print(df_h2b.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visas = pd.concat([df_h1b_listed, df_h2a_listed, df_h2b_listed])\n",
    "df_visas.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "h1b_drop_cols = ['Line by line', 'Employer (Petitioner) Name', 'Tax ID', 'Initial Denial', 'Continuing Denial', 'Initial Approval', 'Continuing Approval']\n",
    "#Adding the columns does not work since they are currently strings, to_numeric not working due to commas\n",
    "df_h1b_listed['tot'] = df_h1b_listed['Initial Approval'] + df_h1b_listed['Continuing Approval']\n",
    "df_h1b_clean = df_h1b_listed.drop(h1b_drop_cols, axis = 1)\n",
    "df_h1b_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visas.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['Line by line', 'Employer (Petitioner) Name', 'Tax ID', 'Initial Denial', 'Continuing Denial', 'Index()', 'Consular_processed', 'New Employment Denial', 'Continuation Denial', 'Change with Same Employer Denial', 'New Concurrent Denial', 'Change of Employer Denial', 'Amended Denial', 'Consular_Processed', 'ETA Case Number']\n",
    "df_visas_clean = df_visas.drop(drop_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visas_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def fetch_stock_data(tickers):\n",
    "    all_data = {}\n",
    "    print('Downloading yfinance historical price data 2000/01/01 to 2025/03/16')\n",
    "    num_downloaded = 0\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            data = yf.download(ticker, start=\"2000-01-01\", end=\"2025-03-16\", progress = False)\n",
    "            all_data[ticker] = data\n",
    "            num_downloaded +=1\n",
    "            time.sleep(2.2)  # Sleep for 2 seconds between requests to avoid being blocked\n",
    "        except Exception as e:\n",
    "            print(f\"Error downloading {ticker}: {e}\")\n",
    "            time.sleep(2.2)\n",
    "\n",
    "    print('Successfully downloaded ' + str(num_downloaded/len(tickers)))\n",
    "    #Save a csv for offline work\n",
    "    sp500_data = pd.concat(all_data, axis=1)\n",
    "    sp500_data.to_csv(\"sp500_data.csv\")\n",
    "    \n",
    "    print(\"S&P 500 historical data saved to sp500_data.csv\")\n",
    "    return sp500_data\n",
    "\n",
    "#Only run this once on your system\n",
    "#Make sure NOT to track the sp500_data csv\n",
    "#sp500csv made for offline work\n",
    "#fetch_stock_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "econ570",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
